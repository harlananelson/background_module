---
title: "Basic Statistics Removed Parts"
output: html_notebook
---

## Bayesian decision theory
* Find a classifier f(x) that minimizes expected risk (expected value of loss)

$R[f]= \sum_{x\in X}{\sum{c(x,y_j,f(x))P(y_j,x)}}$
* We want to find f that minimizes this but we don't have all data points.  We only have training data.
* And we don't know P(y,x)

## Emperical Risk
* Since we only have training data we can't calculate the expected risk (we don't even know P(x,y))
* Solution: we approximate P(x,y) with the empirical distribution

$p_{emp}(x,y) = \frac{1}{n}\sum_{i=1}^{n}\delta_{x_i}(x)\delta_{y_i}(y)$
* The delta function = 1 if x=y and 0 otherwise.

## Empirical Risk

$R_{emp}[f] = R[f]= \sum_{x\in X}{\sum_{j=1}^2{c(x,y_j,f(x))P_{emp}(y_j,x)}}$
$\frac{1}{n}\sum_{i=1}^{n}c(x_i,y_i,f(x_i))$
* Once the loss function is defined and training data is given we can then find f that minimizes this.

## Different Empirical Risks
*Linear Regression

$\frac{1}{n}\sum_{i=1}^{n}{(y_i - (w^Tx_i+w_0))^2}$

* Logistic Regression

$\frac{1}{n}\sum_{i=1}^{n}{log(1 + e^{-y_i(w_TG_i + w_0)})}$

* SVM

## Does it make sense to optimize empirical risk?

* Does 

$R_{emp}(f)$
approach R(F) as we increase sampl size?
* Yes according o the law of large numbers: The mean value of a random sample approaches tru mean as the 
sample size increases.
* But how fast does it conversge?

## Chernoff bounds

