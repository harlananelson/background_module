---
title: "Basic Statistics"
output: html_notebook
---

# Basic Statistics

## Basic Probability
* Random  variable
* Proabability of an event
* Coin toss example
* Independent random variables
* Mean and variance of a random variable
* Correlation between random variables
* Probability distributions
* Central Limit Theorem
* The law of large numbers.

## Random Variable
* A variable normally takes on different values
* A Random variable has values with different probabilities
* Coin toss example
* Dice example
* Probabilities must sum to 1

## Probability of an Event
* Sample space and event space

Random variable associates a number between 0 and 1 to each element in the sample space

attach a measure to each mutually exclusive element.
1. Heads: 0.5
2. Tails: 0.5

Interpretation.

Frequency.
If you toss a coin many times, you will get tails about half of the time.

Example: two dice.  Counting.
When you toss one die, there are six possibilities, so the chance of getting
a 1 is 1/6.

Two dice.  Chance of a  the sum of the dots to be equal to 2.
6*6 =36 so there are 36 ways to throw two dice.  
One way results in a 2.  So we have 1/36.

What is the chance of the sum being 3?
2: 1
3: 2
4: 3
5: 4
6: 5
7: 6
8:  5
9:  4 
10: 3
11: 2
12: 1

Does it add up to 36?

P(A) = .25
P(B) = .25
$.25 <= P(AUB) <= .5$
LaTeX
$0 \le P(A\cap B) \le .25$

P(A|B)  Probability of A given B.
The universe has a probability of 1.
P(U) = 1
P(B) = P(B|U)

What is the change that you role a 4 given that one die is a 3?
P(4|one die is a 3)

What is the deominator.
How many ways can one die be a 3?
There are 12 ways.
3,1   1,3
2/12

Multiply the probability of two independent events to get the
probability of  both event occuring

```{r}
1/6 * 5/6
5/36
```
5/36


6,1  1,6  2,5   5,2  3,4  4,3  





No discrete measurement?
attach probabilities to intervals.
What is the chance it will rain between 3 and 4 inches.



Mutually exclusive:  Heads and tails are mutually exclusive.
We can add probabilities of events that are mutually exclusive.



* Proability of an event
* Counting 
* Probability and counting
* Coint tossing example problems

## Basic Probability
* Independent events: coin toss example
* Mean and variance
* Correlation coefficient (also Pearson's correlation coefficient)
* Formulas:
              
$\mathrm{Covariance(X,Y)} = E[(X-\mu_X)(Y - \mu_Y)]$

$\mathrm{Correlation(X,Y)} = \mathrm{Covariance(X,Y)}/\sigma_x \sigma_Y$

$\mathrm{Pearson~Correlation} = \rho = \frac{\sum_{i=1}^n{(X_i-\bar{X})}(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n(X-\bar{X})^2}\sqrt{\sum_{i=1}^n(Y-\bar{Y})^2}}$


* Bernoulli trials
* Binomial distribution
* Gaussian distribution
* Chi-square distribution
* Law of large numbers: empirical mean converges to true mean as we do more trials
* Central limit theorem: avarage of sampling distribution converges to the average of one as we do more trials.
* Bayes rule
* Posterior probability and likelihood
* Bayesian decision theory
* Loss functions, expected reisk, and empirical risk
* Hypothesis testing and log likelihood ratio
* Neyman-Person lemma, Wilks' theorem
* Chi-square test, Person correlation coefficiant test

## Bayes Rule
* Fundamental to statistical inference
* Conditional Probability
* Posterior = (likelihood * Prior)/ Normalization Constant

$P(M|X) = \frac{P(X|M)P(M)}{P(X)}$ = $\frac{P(X|M)P(M)}{\{\sum_M{P(X|M)P(M)}}$

## Hyposthesis Testing
* We can use Bayes rule to help make decisions
* An outcome or action is described by a model
* Given two models we pick the one with the higher probability
* Coin toss example: use likelihood to determin which coin generated the tosses.

## Maximum likelihood example
* Consider as set of coin tosses produced by a coin with 

$P(H)=P$ and $P(T) = 1-P$
* We want to determine the probabilty P(H) of the coin the  produces k heads and n-k tails.
* We are given some tosses (training data): HTHHHTHHHTHTH
* Solution:
    - Form the log likelihood
    - Differentiate w.r.t. P
    - Set  the derivative to 0 and solve for p
    
# Maximum Likelihood exmple
* Likelihood is the probability of data given the model
* Data are the set of coin tosses and the model is given by one parameter p
* P(data|p) = 

$p^k(1-p)^{n-k}$

$\log{P(data|p)} = log(p^k) = log(1-p)^{n-k} = k log(p) + (n -k)log((1-p)$
*Take the derivative with respect to p, set it to 0, and solve for p

## The Person correlation coefficient
* Measures the correlation between two variables.

$\mathrm{Pearson~Correlation} = \rho = \frac{\sum_{i=1}^n{(X_i-\bar{X})}(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n(X-\bar{X})^2}\sqrt{\sum_{i=1}^n(Y-\bar{Y})^2}}$

* The correlation r is between -1 and 1.  A value of 1 means perfect positive correlation and
-1 in the other direction
* The value f(r) is distributed as a t-distribution that can be used to obtain a p-value.
```{r}
library(magrittr)
library(ggplot2)
```
```{r}
x_mean<-5
y_mean<-10
sd_x <-3
sd_y <- 4
p <- -1
corr <- p*sd_x*sd_y
sig_ <- matrix(c(sd_x^2,corr,corr,sd_y^2),nrow=2)
dimnames(sig_) <- list(c("X","Y"),c("X","Y"))
print(sig_)
m <- matrix(c(9,-13,-13,16),nrow=2)
m

d<-MASS::mvrnorm(10000,mu=c(x_mean,y_mean),Sigma=m)
cov(d)
```

```{r}
f1 <- function(p,x_mean=5,y_mean=10,sd_x=3,sd_y=4) {
  #' Simulate some data for a bivariate normal distribution.
  #' X has variance
  #' Y has variance
  #' There is correlation between X and Y
  corr <- p*sd_x*sd_y
  sig_ <- matrix(c(sd_x^2,corr,corr,sd_y^2),nrow=2)
  MASS::mvrnorm(10000,mu=c(x_mean,y_mean),Sigma=sig_)
}
d<-f1(.5)
dimnames(d) <- list(NULL,c("x","y"))
as.data.frame(d) %>%
  ggplot() +
  aes(x,y) +
  geom_smooth(method='lm') +
  geom_point()


```
```{r}
a <- 9 * 16 * .5
a

MASS::mvrnorm(10,mu=c(1,2),Sigma=matrix(c(9,12,12,16),nrow=2))
```

```{r}
library(magrittr)
library(ggplot2)
```

```{r}
as.data.frame(d) %>%
  ggplot() +
  aes(x,y) +
  geom_smooth(method='lm') +
  geom_point()


```


$P(A \cap B) $
$P(A|B)P(B)$
$P(B|A)P(A)$
$P(A|B) = \frac{P(A \cap B)}{P(B)$
$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
```{r}

```

Conditional prob 

i




